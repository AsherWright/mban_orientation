---
title: "Statistical Modeling & Analysis in `R`"
subtitle: "Machine Learning with Boston AirBnB data"
author: "Colin Pawlowski"
date: "8/28/2018"
output: 
  html_document:
    theme: sandstone
    highlight: tango
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Overview
Now that you have seen data wrangling in `R`, which is a necessary first step in any analytics project, we have the building blocks in place to do some more advanced data analytics.  In this module, we will build machine learning models using the AirBnB data set.  We will cover *regression* models, which can be used to predict continuous variables such as `price`, and *classification* models, which can be used to predict categorical variables such as `amenity_Elevator_in_building`.  In the end, you will build your own models and add these to your business analytics dashboard to improve its value.  

# Predicting AirBnB Prices

First, let's load the `tidyverse` package and read in the data set which has been pre-processed for machine learning for us. If you are interested, you can check out the file `airbnbDataPrep.html` to see how this was done.  
```{r readdata}
library(tidyverse)
listings <- readRDS("../data/listingsML.RDS")
```

## 1. Linear Regression
Before we build a linear model to predict price, we'll reserve
a portion of our data to be a test set. There are lots of ways
to do this. We'll use the `caTools` package.

```{r splitData}
# install.packages("caTools")
library(caTools)
set.seed(123)
spl <- sample.split(listings$price, SplitRatio = 0.7)
listingsTrain <- listings[spl,]
listingsTest <- listings[!spl,]
```

We now have two data frames which contain the training and
testing data, respectively.  The command `set.seed(123)`
ensures that our random splits will be the same, i.e. there is a random
number generator in `R` initialized with the value 123.
The function `sample.split(y, SplitRatio = 0.7)`
generates a random vector of 70\% `TRUE`s,  30\% `FALSE`s
which is the same length as the vector `y`.  If
`y` has few discrete classes, it preserves the ratio
classes in both sets as well.  

### (a) Model Training
In `R`, we specify a model structure and then use the corresponding
function to tell `R` to optimize for the best-fitting model.
For linear regression, the function is `lm()`:
```{r lm1}
lm1 <- lm(price ~ accommodates, data = listingsTrain)
```

Here, the \"~\" notation creates a formula object in R.
On the left side, we put the dependent variable, and on
the right hand side we put the independent variables, separated by \"+\" signs.  

Let's check out the lm1 object. It is a list of a bunch of relevant
information generated by the `lm()` function call, and we can use
the `$` to view different elements. The function `summary()` is overloaded
for many different objects and often gives a useful snapshot of the model as well.
```{r view_lm1}
names(lm1)
lm1$coefficients
summary(lm1)
```

First, let's look at the section under \"Coefficients\". 
Notice that `R` automatically adds an intercept
term unless you tell it not to (we'll see how to do this later).
In the \"estimate\" column, we see that the point estimates for the
linear model here say that the price is \$55.20 plus \$37.79 for
every person accommodated. Notice the '***' symbols at the end of
the \"(Intercept)\" and \"accommodates\" rows. These indicate that
according to a statistical t-test, both coefficients are extremely
significantly different than zero, so things are okay from an
inference perspective.	

### (b) Model Visualization
As another check on inference quality, let's plot the fitted line.
There are some nifty functions in the `modelr` package that make
interacting with models easy in the `tidyr` and `dplyr` setting.
We'll use `modelr::add_predictions()` here.	
```{r plotFit}
# install.packages("modelr")
library(modelr)
listingsTrain %>%	
  add_predictions(lm1) %>%	
  ggplot(aes(x = accommodates)) +	
  geom_point(aes(y = price)) +	
  geom_line(aes(y = pred), color = 'red')
```

Nice. We can also remove the linear trend and check the residual
uncertainty, which we'll do here using `modelr::add_residuals()`.
This is helpful to make sure that the residual uncertainty looks
like random noise rather than an unidentified trend.	
```{r addResiduals}
listingsTrain %>%	
  add_residuals(lm1, var = "resid") %>%	
  group_by(as.factor(accommodates)) %>%	
  ggplot(aes(x = as.factor(accommodates), y = resid)) + 
  geom_boxplot()
```

Things are pretty centered around zero, with the exception of
9+ person accommodations. Maybe the model doesn't apply
so well here, why might that be?

### (c) Model Evaluation
Now, what if we wanted to *quantify* how well the model predicts
these out-of-sample values? We'll look at the out-of-sample R^2 (OSR^2),
also known as the \"coefficient of determination\":

OSR^2 = 1 - SSE / SST,

where:

* SSE = $\sum_{i=1}^n (\hat{y}_i - y_i)^2$  (\"Sum of Squares Error\")

* SST = $\sum_{i=1}^n (\bar{y} - y_i)^2$  (\"Sum of Squares Total\")

In these equations, $\hat{y}_i$ is the predicted value for test
observation $i$, $y_i$ is the actual value, $n$ is the size of
the test set, and $\bar{y}$ is the mean of $y_i$ in the training set.
Let's code this up.
```{r OSR2}
pred_test <- predict(lm1, newdata = listingsTest)
OSR2 <- 1 - sum((pred_test - listingsTest$price) ^ 2) /
  sum((mean(listingsTrain$price) - listingsTest$price) ^ 2)
```

The in-sample R^2 value is equal to the \"Multiple R-squared\"
value returned by summary().  Are we overfitting?
```{r overfitting}
summary(lm1)
OSR2
```

### (d) First Set of Exercises
**1. Building a simple model**

Regress price on review_scores_rating. Plot the regression
line and the actual training points, and find the in-sample
R^2. (Read below for more details if you need them.)

**DETAILS:**

- Use `lm()` to learn the linear relationship
- In-sample R^2 is one of the outputs of `summary()`
- Use `add_predictions()` and ggplot tools for the plotting

**2. Adding more variables**

Try to beat the out-of-sample performance of the
price ~ accommodates model by adding other variables. You can use
`names(listings)` to explore potential predictors.
If you start getting  errors or unexpected behavior, make sure
the predictors are in the format you think they are.
You can check this using the `summary()` and `str()` functions
on listings$\<variable of interest\>. 

**3. Median Regression**

Since we're dealing with data on price,
we expect that there will be high outliers. While least-squares
regression is reliable in many settings, it has the property 
that the estimates it generates depend quite a bit on the outliers.
One alternative, median regression, minimizes *absolute* error
rather than squared error. This has the effect of regressing
on the median rather than the mean, and is more robust to outliers.
In `R`, it can be implemented using the `quantreg` package.

For this exercise, install the quantreg package, and compare
the behavior of the median regression fit (using the `rq()`)
function) to the least squares fit from `lm()` on the original
listings data set given below which includes all the price outliers.
```{r exercise1.3}
data <- readRDS("../data/listingsMLwithOutliers.RDS")
```
*Hint:* Enter ?rq for info on the rq function.

**DETAILS:**
  
- Split into training/testing set
- Fit the median and linear regression models
- Plot the two lines together using `gather_predictions`,
which is very similar to the `add_predictions` function
that we saw in class. 
- Add "color = model" as a geom_line aesthetic
to differentiate the two models in the plot.

### (e) Summary and More about Formulas

First, let's review the pattern, because it can be generalized to
a whole bunch of more complex models. We asked the questions:
How does listing price depend on the number of people it
accommodates? How well does accommodation size predict price?
Since we were interested in prediction, we reserved part of our
data as a test set. We then chose to use a linear model to
answer these questions, and found the corresponding function `lm()`.
This function, and modelling functions in general, takes as arguments:

* Data on the response and predictor variables,
usually through a `formula` object	
* Model parameters (in the case of `lm()`, we used all
the default values)	

`R` then automatically found the "best" linear model by computing
the least squares estimate, and returned a `lm` object, which
was a list including information about:

* Fitted coefficients	
* Residuals	
* Statistical significance	
* And more...	

We interacted with the model to evaluate goodness-of-fit and
out-of-sample performance. In our case, we used the `caTools`
and `dplyr` framework to do this cleanly.	

We didn't say too much about the `price ~ accommodates` syntax.
Many modelling functions in `R` take `formula`s as arguments,
together with a `data` argument. The `data` argument specifies
the data frame, and the `formula` argument tells the model
which are the responses and which are the predictors. We'll
play around with formulas in the exercises, but here are a
few helpful pointers:	

* The `~` separates the response (on the left) from the
predictors (on the right)	
* Predictors are separated with a `+`	
* Use `.` on the right-hand side to include all predictors
in a given data frame	
* You can also use `.-x` to include all predictors except `x`	
* To include interactions between variables, use the `*` symbol.
For example: `y ~ x + z + x*z` expresses the form:
"regress `y` on `x`, `z`, and on `x` interacted with `z`	
* To exclude the intercept term, include `-1` or `+0` on
the right-hand side	
For more detailed info, see
<https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html>.	

### (f) Adding More Features
Let's work a bit harder on predicting price, this time using more
than one predictor. In fact, we'll add a bunch of predictors to
the model and see what happens.	We can do this using the `.` notation
in our formula:
```{r moreFeatures}
lm2 <- lm(price ~ ., data = listingsTrain)
```

```{r OSR2lm2}
pred_test <- predict(lm2, newdata = listingsTest)
OSR2_2 <- 1 - sum((pred_test - listingsTest$price) ^ 2) /
  sum((mean(listingsTrain$price) - listingsTest$price) ^ 2)
```

The in-sample R^2 value is equal to the \"Multiple R-squared\"
value returned by summary().  Are we overfitting?
```{r overfitting_lm2}
summary(lm2)$r.squared
OSR2_2
```

This has better in-sample performance than our initial model,
but we have a serious overfitting problem here, meaning that the training error
is smaller than the test error. The model is too powerful for the amount of data we have.
Sometimes, `R` recognizes this by giving warnings about a \"rank-deficient fit.\"	


## 2. Regularized Linear Regression
But is there still a way to use the info from all these
variables without overfitting? Yes! One way to do this is by
regularized, or penalized, regression.	

Mathematically, we add a term to the optimization problem that
we're solving when fitting a model, a term which penalizes models
that get too fancy without enough data. If we call $\beta$ the
coefficient vector that we'd like to learn about for linear
regression, then the regular regression we've worked with
looks like	
$$	
\min_\beta \sum_{i=1}^n (y_i-x_i^T\beta)^2,	
$$	
but penalized regression looks like	
$$	
\min_\beta \sum_{i=1}^n (y_i-x_i^T\beta)^2 + \lambda ||\beta||.	
$$	

There are two types of flexibility within this framework:

* Choice of norm, a structural decision, and	
* Choice of $\lambda$, a parametric decision.	

Two natural choices of norm are the Euclidean 1- and 2-norms.
When we use the 2-norm, it's often called "ridge regression."
We'll focus today on the 1-norm, or "LASSO regression". On a
very simple level, both types of regression shrink all the
elements of the unconstrained $\beta$ vector towards zero,
some more than others in a special way. LASSO shrinks the coefficients
so that some are equal to zero. This feature is nice because it helps
us interpret the model by getting rid of the effects of many
of the variables.	

To do LASSO, we'll use the `glmnet` package. Of note, this package
doesn't work very elegantly with the `tidyverse` since it uses
matrix representations of the data rather than data frame
representations. However, it does what it does quite well, and
will give us a chance to see some base R code. Let's load the
package and check out the function `glmnet()`. We can see the
documentation from the command line using `?glmnet`.	
```{r glmnetPackage}
library(glmnet)
?glmnet
```

Notice that `glmnet()` doesn't communicate with the data via
formulas. Instead, it wants a matrix of predictor variables and
a vector of values for the variable we're trying to predict,
including all the categorical variables that `R` automatically
expanded into indicator variables. Fortunately, `R` has a
`model.matrix()` function which takes a data frame and gets it
into the right form for `glmnet()` and other functions with this
type of input.	

First, we need to convert the data to a matrix for X and a vector for Y:
```{r modelMatrix}
x <- model.matrix(price ~ ., data = listings)
y <- as.vector(listings$price)
```

Next, split into training/testing sets:
```{r splitLasso}
set.seed(123)
spl <- sample.split(y, SplitRatio = 0.7)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
```

### (a) Model Training
Finally, let's fit a our first LASSO model. There's a way to
specify lambda manually, but let's just accept the default
for now and see what we get.
```{r lasso1}
lasso1 <- glmnet(xTrain, yTrain)
```

This time the `summary()` function isn't quite as useful:
```{r lassoSummary}
summary(lasso1)
```

It does give us some info, though. Notice that "lambda" is a
vector of length 90. The `glmnet()` function has defined 90
different values of lambda and found the corresponding optimal
beta vector for each one! We have 90 different models here.
Let's look at some of the coefficients for the different models.
We'll start with one where lambda is really high:
```{r lassoHighLambda}
lasso1$lambda[1]
lasso1$beta[, 1] # How many coefficients are nonzero?	
```

Here the penalty on the size of the coefficients is so high
that `R` sets them all to zero. Moving to some smaller lambdas:	
```{r smallerLambda}
lasso1$lambda[10]	
beta <- lasso1$beta[, 10] 
beta[which(beta != 0)]
	
lasso1$lambda[20]
beta <- lasso1$beta[, 20]
beta[which(beta != 0)]
```

### (b) Cross-Validation
How do we choose which of the 90 models to use? Or in other words,
how do we "tune" the $\lambda$ parameter? We'll use a similar idea
to the training-test set split called cross-validation.	

The idea behind cross-validation is this: what if we trained our
family of models (in this case 90) on only some of the training data
and left out some other data points? Then we could use those other
data points to figure out which of the lambdas works best
out-of-sample. So we'd have a training set for training all the
models, a validation set for choosing the best one, and a test set
to evaluate performance once we've settled on a model.	

There's just one other trick: since taking more samples
reduces noise, could we somehow take more validation set
samples? Here's where *cross*-validation comes in.
We divide the training data into groups called *folds*,
and for each fold repeat the train-validate procedure on
the remaining training data and use the current fold as a
validation set. We then average the performance of each model
on each fold and pick the best one.	

```{r, out.height = 300, fig.retina = NULL, echo = FALSE, }
knitr::include_graphics("figures/crossValidation.png")
```

This is a very common *resampling* method that applies in lots and
lots of settings. Lucky for us the glmnet package has a very handy
function called `cv.glmnet()` which does the entire process
automatically. Let's look at the function arguments using `?cv.glmnet`.	

The relevant arguments for us right now are

* x, the matrix of predictors	
* y, the response variable	
* nfolds, the number of ways to split the training set (defaults to 10)
* type.measure, the metric of prediction quality. It defaults to
mean squared error, the square of RMSE, for linear regression.

Let's do the cross-validation.  Because the cross-validation procedure
is inherently randomized, let's set the seed beforehand to obtain consistent results.
```{r lassoCV}
set.seed(123)
lasso2 <- cv.glmnet(xTrain, yTrain)
summary(lasso2) # What does the model object look like?	
```

Notice the \"lambda.min\". This is the best lambda as determined by
the cross validation. \"lambda.1s\" is the largest lambda such that
the \"error is within 1 standard error of the minimum.\"	

There's another automatic plotting function for `cv.glmnet()`
which shows the error for each model:	
```{r lassoPlotCV}
plot.cv.glmnet(lasso2)
```

The first vertical dotted line shows `lambda.min`, and the
second is `lambda.1se`. The figure illustrates that we cross-validate
to find the "sweet spot" where there's not too much bias (high lambda)
and not too much noise (low lambda). The left-hand side of this graph
is flatter than we'd sometimes see, meaning that the unpenalized model
may not be too bad. However, increasing lambda increases
interpretability at close to no loss in prediction accuracy!	

### (c) Model Evaluation
Let's again compare training and test error. Because we are using
glmnet we need to use the specialized `predict.cv.glmnet()` function
to get the in-sample and out-of-sample predictions:
```{r predictLassoCV}
?predict.cv.glmnet
pred_train <- predict.cv.glmnet(lasso2, newx = xTrain, s = "lambda.1se")
pred_test <- predict.cv.glmnet(lasso2, newx = xTest, s = "lambda.1se")
```

Then use the formula to compute R^2 and OSR^2:
```{r lassoCV_R2}
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
  sum((mean(yTrain) - yTrain) ^ 2)
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
  sum((mean(yTrain) - yTest) ^ 2)
R2_lasso
OSR2_lasso
```

Has the overfitting issue improved?  Let's check out the coefficients
in our final model:
```{r lassoCVcoefficients}
lambdaIndex <- which(lasso2$lambda == lasso2$lambda.1se)
beta <- lasso2$glmnet.fit$beta[,lambdaIndex]
beta[which(beta != 0)]
lasso2$lambda.1se
```

The overfitting problem has gotten better, but hasn't yet gone
away completely. I added a bunch variables for dramatic effect
that we could probably screen out before running the LASSO if we
really wanted a good model.	

One more note on cross-validation: the `glmnet` package has built-in
functionality for cross-validation. In situations where that's not
the case, `modelr::crossv_kfold()` will prepare data for
cross-validation in a nice way.	

### (d) Second Set of Exercises

**1. Elastic-Net Regression**

The glmnet package is actually more versatile than just
LASSO regression. It also does ridge regression (with the l2 norm),
and any mixture of LASSO and ridge (also known as elastic-net).
The mixture is controlled by the parameter alpha: alpha=1 is the default and corresponds
to LASSO, alpha=0 is ridge, and values in between are mixtures
between the two (check out the formula using ?glmnet).
One could use cross validation to choose this
parameter as well. For now, try just a few different values of
alpha on the model we built for LASSO using `cv.glmnet()`
(which does not cross-validate for alpha automatically).
How do the new models do on out-of-sample R^2?

# Predicting AirBnB Elevator Access

So far, we have looked at models for predicting price, which is
a continuous variable.  For this type of task, we use *regression* methods.
However, if we want to predict a variable which is categorical,
we must use *classification* methods.  Let's take a look at using
one such method, **Logistic Regression**, to predict the binary variable
whether or not the AirBnB includes an elevator.  

## 1. Logistic Regression
Logistic regression is part of the class of generalized linear
models (GLMs), which build directly on top of linear regression.
These models take the linear fit and map it through a non-linear
function. For logistic regression this function is the logistic
function,

$$f(x) = \frac{e^x}{1+e^x},$$

which looks like this:
```{r logRegExample}
x <- seq(-10, 10, 0.25)
y <- exp(x) / (1 + exp(x))
plot(x, y)
```

Since the function stays between zero and one, it can be interpreted
as a mapping from predictor values to a probability of being in one
of two classes.	

Let's apply this model to the `listings` data. Let's try to predict
which listings have elevators in the building by price.  First,
we need to split the data into training and testing sets. 

```{r elevatorSplit}
set.seed(123)
spl <- sample.split(listings$amenity_Elevator_in_Building, SplitRatio = 0.7)
elevatorTrain <- listings[spl,]
elevatorTest <- listings[!spl,]
```

One nice thing about using `sample.split` for classification is that
it preserves the ratio of class labels in the training and testing sets.

### (a) Model Training
Instead of the `lm()` function, we'll now use `glm()`, but the
syntax is almost exactly the same:
```{r logReg}
logReg1 <- glm(amenity_Elevator_in_Building ~ price,
            family = "binomial", data = elevatorTrain)
summary(logReg1)
```

### (b) Model Accuracy
Let's explore out-of-sample performance. 
To evaluate the prediction accuracy of our logistic regression model,
we count up the number of times each of the following occurs:

* Y = 1, prediction = 1 (True Positive)
* Y = 0, prediction = 1 (False Positive)
* Y = 1, prediction = 0 (False Negative)
* Y = 0, prediction = 0 (True Negative)

A table that holds these values is called a "confusion matrix". Then, 

accuracy = (# True Positives + # True Negatives) / (Total # of observations)

Let's construct the confusion matrix and calculate
the training and testing accuracy for our model:
```{r confusionMatrix}
# Training Confusion Matrix
pred_train <- predict(logReg1, newdata = elevatorTrain)
confusionMatrixTrain <- table(elevatorTrain$amenity_Elevator_in_Building,
                         ifelse(pred_train > 0.5, "pred = 1", "pred = 0"))
confusionMatrixTrain
# Training Accuracy
sum(diag(confusionMatrixTrain)) / nrow(elevatorTrain)
# Testing Confusion Matrix
pred_test <- predict(logReg1, newdata = elevatorTest)
confusionMatrixTest <- table(elevatorTest$amenity_Elevator_in_Building,
                         ifelse(pred_test > 0.5, "pred = 1", "pred = 0"))
confusionMatrixTest
# Testing Accuracy
sum(diag(confusionMatrixTest)) / nrow(elevatorTest)
```

Are we overfitting with this model?

### (c) Model AUC
Ultimately, we want to predict whether or not a listing has an
elevator. However, logistic regression gives us something a bit
different: a probability that each listing has an elevator. This
gives us flexibility in the way we predict. The most natural
thing would be to predict that any listing with predicted
probability above 0.5 *has* an elevator, and any listing with
predicted probability below 0.5 *does not have* an elevator. But
what if I use a wheelchair and I want to be really confident that
there's going to be an elevator? I may want to use a cutoff value
of 0.9 rather than 0.5. In fact, we could choose any cutoff value
and have a corresponding prediction model.  How should tuning this value
affect the False Postive and False Negative rates?

There's a really nice metric that measures the quality of all
cutoffs simultaneously: *AUC*, for "Area Under the receiver
operating characteristic Curve." That's a mouthful, but the idea
is simpler: For every cutoff, we'll plot the *false positive rate*
against the *true positive rate* and then take the area under this
curve. (A *positive* in our case is a listing that has an elevator.
So a *true positive* is a listing that we predict has an elevator
and really does have an elevator, while a *false positive* is a
listing that we predict has an elevator and does *not* actually
have an elevator.)	

As the cutoff shrinks down from 1 to 0, the rate of total positives
will increase. If the rate of true positives increases faster than
the rate of false positives, this is one indication that the model
is good. This is what AUC measures.	

The `ROCR` package is one implementation that allows us to plot
ROC curves and calculate AUC. Here's an example:	
```{r logRegAUC}
# install.packages("ROCR")
library(ROCR)
pred_test <- predict(logReg1, newdata = elevatorTest, type = "response")	
pred_obj <- prediction(pred_test, elevatorTest$amenity_Elevator_in_Building)

# Creating a prediction object for ROCR
perf <- performance(pred_obj, 'tpr', 'fpr')	
plot(perf, colorize = T) # ROC curve
performance(pred_obj, 'auc')@y.values[[1]] # AUC - a scalar measure of performance	
```

As you can see, the `performance()` function in the `ROCR` package
is versatile and allows you to calculate and plot a bunch of
different performance metrics.

In our case, this model gives an AUC of 0.7. The worst possible
is 0.5 - random guessing. We're definitely better than random here,
and could likely improve by adding more predictors.	

We've covered basic logistic regression, but just as with linear
regression there are many, many extensions. For example, we could
do regularized logistic regression if we wanted to use many predictors,
using the `glmnet` package.	

### (d) Third Set of Exercises

**1. Add more variables to Logistic Regression**

Try to beat the out-of-sample performance for logistic
regression of elevators on price by adding new variables.
Compute the out-of-sample AUC of the final model,
and plot the ROC curve.  

# Adding Predictive Models to the Business Dashboard

Now that you have been introduced to statistical modelling
in `R`, it's time to take these tools and use them to build
your own predictive model as a module for the dashboard.
In the process, you may consider the following questions:

- What is the outcome variable that you would like to predict?
What are the independent variables?
- Who is the target audience for your
predictive model?  (This could be AirBnB, landlords, renters, etc.)
- What are some key questions that your target
audience might have?  How does your predictive model provide
insight to address these questions?
- How could your target audience use the output of your
model to change their behavior to improve some objective?
- How could you improve or change this predictive model?  If
you could add more data, what would it be?

For this part, you will work in the same teams as before.
If there is time remaining at the end of the session,
we will have groups present their dashboards
for short 1 minute presentations.  
